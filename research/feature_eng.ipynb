{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fifty\\\\OneDrive\\\\Desktop\\\\AI - Data Science\\\\Mlops\\\\Malicious_QR_Code_Detection\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fifty\\\\OneDrive\\\\Desktop\\\\AI - Data Science\\\\Mlops\\\\Malicious_QR_Code_Detection'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FeatureEngineeringConfig:\n",
    "    root_dir: Path\n",
    "    csv_file: str\n",
    "    train_data: str\n",
    "    test_data: str\n",
    "    scaler_file: str\n",
    "    schema: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.MaliciousQRCodeDetection.constants import *\n",
    "from src.MaliciousQRCodeDetection.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_feature_engineering_config(self) -> FeatureEngineeringConfig:\n",
    "        config = self.config.feature_engineering\n",
    "        schema = self.schema\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        feature_engineering_config = FeatureEngineeringConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            csv_file=config.csv_file,\n",
    "            train_data = config.train_data,\n",
    "            test_data = config.test_data,\n",
    "            scaler_file = config.scaler_file,\n",
    "            schema = schema\n",
    "        )\n",
    "\n",
    "        return feature_engineering_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.MaliciousQRCodeDetection.logging.logger import logger\n",
    "from src.MaliciousQRCodeDetection.exception import MaliciousQRException\n",
    "from tld import get_tld\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    def __init__(self,config: FeatureEngineeringConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def extract_url_domain(self,url):\n",
    "        try:\n",
    "            res = get_tld(url, as_object=True, fail_silently=False, fix_protocol= True)\n",
    "            pri_domain = res.parsed_url.netloc\n",
    "        except:\n",
    "            pri_domain = None\n",
    "\n",
    "        return pri_domain\n",
    "    \n",
    "    def url_len(self,url):\n",
    "        length = str(len(url))\n",
    "        return length\n",
    "    \n",
    "\n",
    "    def httpSecure(self,url):\n",
    "        https = urlparse(url).scheme\n",
    "        match = str(https)\n",
    "\n",
    "        if match == 'https':\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def no_of_digits(self,url):\n",
    "        digits = 0\n",
    "        for i in url:\n",
    "            if i.isnumeric():\n",
    "                digits+=1\n",
    "\n",
    "        return digits\n",
    "    \n",
    "    def letter_count(self,url):\n",
    "        letter = 0\n",
    "        for i in url:\n",
    "            if i.isalpha():\n",
    "                letter+=1\n",
    "\n",
    "        return letter\n",
    "    \n",
    "    def Shortining_Service(self,url):\n",
    "        match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                          'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                          'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                          'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                          'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                          'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                          'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                          'tr\\.im|link\\.zip\\.net',\n",
    "                          url)\n",
    "        if match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def having_ip_address(self,url):\n",
    "        match = re.search(\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4 with port\n",
    "            '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)' # IPv4 in hexadecimal\n",
    "            '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'\n",
    "            '([0-9]+(?:\\.[0-9]+){3}:[0-9]+)|'\n",
    "            '((?:(?:\\d|[01]?\\d\\d|2[0-4]\\d|25[0-5])\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d|\\d)(?:\\/\\d{1,2})?)', url)  # Ipv6\n",
    "        if match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    #first directory length    \n",
    "    def fd_length(self,url):\n",
    "        urlpath = urlparse(url).path\n",
    "        try:\n",
    "            return len(urlpath.split('/')[1])\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def no_of_dir(self,url):\n",
    "        urldir = urlparse(url).path\n",
    "        return urldir.count('/')\n",
    "    \n",
    "    def process_urls(self,df):\n",
    "\n",
    "        feature = ['@','?','-','=','.','#','%','+','$','!','*',',','//']\n",
    "\n",
    "        df['url_len'] = df['url'].apply(lambda i: self.url_len(i))\n",
    "        df['httpSecure'] = df['url'].apply(lambda i: self.httpSecure(i))\n",
    "        df['no_of_digits'] = df['url'].apply(lambda i: self.no_of_digits(i))\n",
    "        df['letter_count'] = df['url'].apply(lambda i: self.letter_count(i))\n",
    "        df['Shortining_Service'] = df['url'].apply(lambda i: self.Shortining_Service(i))\n",
    "        df['having_ip_address'] = df['url'].apply(lambda i: self.having_ip_address(i))\n",
    "        df['path_length'] = df['url'].apply(lambda i: len(urlparse(i).path))\n",
    "        df['fd_length'] = df['url'].apply(lambda i: self.fd_length(i))\n",
    "        df['count-www'] = df['url'].apply(lambda i: i.count('www'))\n",
    "        df['count-dir'] = df['url'].apply(lambda i: self.no_of_dir(i))\n",
    "\n",
    "        for a in feature:\n",
    "            df[a] = df['url'].apply(lambda i: i.count(a))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # get csv and read in dataframe format\n",
    "    def train_test_split(self):\n",
    "\n",
    "        csvPath = self.config.csv_file\n",
    "        logger.info('csv file downloaded successfully for feature engineering')\n",
    "\n",
    "        df = pd.read_csv('balanced_urls_1.csv')\n",
    "        logger.info('csv to DataFrame done..')\n",
    "\n",
    "        df = self.process_urls(df)\n",
    "        logger.info('all features extracted successfully..')\n",
    "\n",
    "        df = df.drop(columns=['url','label'])\n",
    "        logger.info('unnecessay columns dropped successfully..')\n",
    "\n",
    "        train,test = train_test_split(df,test_size=0.2,random_state=42)\n",
    "        logger.info('Train Test split successfully..')\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train.drop(columns=['result'])\n",
    "        y_train = train['result']\n",
    "        X_test = test.drop(columns=['result'])\n",
    "        y_test = test['result']\n",
    "        logger.info('X_train,y_train,X_test,y_test splitted sucessfully..')\n",
    "\n",
    "        # scale the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        logger.info('Data scaled sucessfully..')\n",
    "\n",
    "        #combine scaled features with target column for saving\n",
    "        train = pd.DataFrame(X_train_scaled,columns=X_train.columns)\n",
    "        train['result'] = y_train.reset_index(drop=True)\n",
    "\n",
    "        test = pd.DataFrame(X_test_scaled,columns=X_test.columns)\n",
    "        test['result'] = y_test.reset_index(drop=True)\n",
    "        logger.info('New scaled train and test df made sucessfully..')\n",
    "\n",
    "        train.to_csv(self.config.train_data, index=False)\n",
    "        test.to_csv(self.config.test_data,index=False)\n",
    "        logger.info('New scaled train and test df saved sucessfully..')\n",
    "\n",
    "        dump(scaler,self.config.scaler_file)\n",
    "        logger.info('scaler.joblib saved sucessfully..')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-12-29 15:24:46,197] 17 root - INFO - yaml file: config\\config.yaml loaded successfully\n",
      "[ 2024-12-29 15:24:46,202] 17 root - INFO - yaml file: schema.yaml loaded successfully\n",
      "[ 2024-12-29 15:24:46,203] 31 root - INFO - Created directory at: artifacts\n",
      "[ 2024-12-29 15:24:46,204] 31 root - INFO - Created directory at: artifacts/feature_engineering\n",
      "[ 2024-12-29 15:24:46,204] 111 root - INFO - csv file downloaded successfully for feature engineering\n",
      "[ 2024-12-29 15:24:46,775] 114 root - INFO - csv to DataFrame done..\n",
      "[ 2024-12-29 15:25:22,859] 117 root - INFO - all features extracted successfully..\n",
      "[ 2024-12-29 15:25:22,922] 120 root - INFO - unnecessay columns dropped successfully..\n",
      "[ 2024-12-29 15:25:23,087] 123 root - INFO - Train Test split successfully..\n",
      "[ 2024-12-29 15:25:23,139] 130 root - INFO - X_train,y_train,X_test,y_test splitted sucessfully..\n",
      "[ 2024-12-29 15:25:24,551] 136 root - INFO - Data scaled sucessfully..\n",
      "[ 2024-12-29 15:25:24,567] 144 root - INFO - New scaled train and test df made sucessfully..\n",
      "[ 2024-12-29 15:25:38,957] 148 root - INFO - New scaled train and test df saved sucessfully..\n",
      "[ 2024-12-29 15:25:38,963] 151 root - INFO - scaler.joblib saved sucessfully..\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    feature_engineering_config = config.get_feature_engineering_config()\n",
    "    feature_engineering = FeatureEngineering(config=feature_engineering_config)\n",
    "    feature_engineering.train_test_split()\n",
    "except Exception as e:\n",
    "    logger.error(f'Unexpected error occured while downloading file: {e}')\n",
    "    raise MaliciousQRException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_url_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
